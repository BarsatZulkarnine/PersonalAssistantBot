# Memory System Architecture

## üß† Overview

The Memory System gives the voice assistant **long-term memory** across conversations. It remembers facts about you, your preferences, past conversations, and important context.

**Key Features:**
- üíæ **Hybrid Storage**: SQL (structured) + Vector DB (semantic)
- üéØ **Smart Classification**: AI decides what's worth remembering
- üîç **Intelligent Retrieval**: Finds relevant memories for any query
- üí∞ **Cost-Effective**: ~$1/month for 1000 conversations

---

## üèóÔ∏è Architecture Diagram

```
User Input
    ‚Üì
[1] Classify Memory Worth
    ‚îú‚îÄ EPHEMERAL ‚Üí Don't store
    ‚îú‚îÄ CONVERSATIONAL ‚Üí SQL only
    ‚îî‚îÄ FACTUAL ‚Üí SQL + Vector
    ‚Üì
[2] Store in SQL
    ‚îú‚îÄ conversations table (all dialogue)
    ‚îú‚îÄ facts table (important info)
    ‚îî‚îÄ facts_fts (FTS5 keyword index)
    ‚Üì
[3] Store in Vector DB (if FACTUAL)
    ‚îî‚îÄ ChromaDB embeddings
    
When user asks something:
    ‚Üì
[4] Hybrid Retrieval
    ‚îú‚îÄ FTS5 keyword search (SQL)
    ‚îú‚îÄ Vector semantic search
    ‚îî‚îÄ Recent conversations
    ‚Üì
[5] Rank & Deduplicate
    ‚Üì
[6] Inject into AI Prompt
```

---

## üìä SQL vs Vector DB - When to Use What

### SQL Database (SQLite)

**What it stores:**
- ‚úÖ ALL conversations (ephemeral, conversational, factual)
- ‚úÖ Metadata (timestamps, user IDs, tokens)
- ‚úÖ Provenance (which conversation created what)
- ‚úÖ Structured queries (by date, user, category)

**Best for:**
- Exact matches ("What did I say yesterday?")
- Recent conversations
- Structured data (dates, counts, IDs)
- Fast keyword search (via FTS5)

**Example queries:**
```sql
-- Find conversations mentioning "birthday"
SELECT * FROM facts_fts WHERE facts_fts MATCH 'birthday';

-- Get last 10 conversations
SELECT * FROM conversations 
WHERE user_id = 'alice' 
ORDER BY timestamp DESC LIMIT 10;

-- Find all personal facts
SELECT * FROM facts 
WHERE category = 'personal' 
ORDER BY importance_score DESC;
```

**Why SQL?**
- FREE (local SQLite)
- FAST (indexed queries)
- RELIABLE (ACID transactions)
- QUERYABLE (powerful SQL)

---

### Vector Database (ChromaDB)

**What it stores:**
- ‚úÖ Embeddings of FACTUAL information only
- ‚úÖ Semantic meaning of content
- ‚úÖ Allows "similar meaning" searches

**Best for:**
- Semantic queries ("Tell me about my preferences")
- Concept-based search ("What do I like?")
- When exact keywords don't match
- Finding related information

**Example queries:**
```python
# Semantic search
vector_store.search(
    query="What does the user enjoy?",
    # Finds: "User loves jazz music"
    # Even though "enjoy" != "love"
)

# Finds semantically similar facts
vector_store.search(
    query="User's age",
    # Finds: "User's birthday is March 15, 1990"
    # Even though "age" not in text
)
```

**Why Vector DB?**
- Understands MEANING, not just keywords
- Finds related concepts
- Language-agnostic (works across languages)
- Great for vague queries

---

## üîÄ Hybrid Retrieval: Best of Both Worlds

The system uses **both** SQL and Vector search simultaneously:

```python
async def retrieve_context(query: str):
    # 1. SQL FTS5 (fast keyword search)
    fts_results = sql_store.search_facts(query)
    # Finds: exact keyword matches
    
    # 2. Vector search (semantic similarity)
    vector_results = vector_store.search(query)
    # Finds: conceptually similar content
    
    # 3. Recent conversations
    recent = sql_store.get_conversations(limit=3)
    
    # 4. Combine, deduplicate, rank
    all_results = fts_results + vector_results + recent
    ranked = rank_by_relevance(all_results)
    
    return ranked[:5]  # Top 5 results
```

**Why Hybrid?**
- **SQL**: Fast, exact, structured
- **Vector**: Semantic, flexible, concept-based
- **Together**: Get the best of both!

---

## üéØ Memory Classification

AI classifies every conversation into one of three tiers:

### Tier 1: EPHEMERAL (Don't Store)
**Criteria:**
- No learning value
- Temporary information
- Generic responses

**Examples:**
- ‚ùå "Hello" ‚Üí "Hi there!"
- ‚ùå "What time is it?" ‚Üí "It's 3:45 PM"
- ‚ùå "Play music" ‚Üí "Playing now"
- ‚ùå "Thanks" ‚Üí "You're welcome"

**Storage:** NONE  
**Cost:** $0

### Tier 2: CONVERSATIONAL (SQL Only)
**Criteria:**
- General dialogue
- No personal information
- Reference value only

**Examples:**
- ‚úì "Tell me a joke" ‚Üí "Why did the..."
- ‚úì "Explain quantum physics" ‚Üí "Quantum physics is..."
- ‚úì "What's for dinner?" ‚Üí "How about pasta?"

**Storage:** SQL conversations table  
**Cost:** ~$0.001/conversation

### Tier 3: FACTUAL (SQL + Vector)
**Criteria:**
- Personal information
- Preferences
- Important context
- Learning content

**Examples:**
- ‚úì‚úì "My name is Alice" ‚Üí Stores in both SQL + Vector
- ‚úì‚úì "I live in Melbourne" ‚Üí Stores in both
- ‚úì‚úì "I prefer jazz music" ‚Üí Stores in both
- ‚úì‚úì "My birthday is March 15" ‚Üí Stores in both

**Storage:** SQL facts table + ChromaDB embeddings  
**Cost:** ~$0.002/fact (one-time)

---

## üìä Database Schema

### SQL Tables

#### `conversations`
Stores ALL dialogue (even ephemeral, for logging):
```sql
CREATE TABLE conversations (
    id INTEGER PRIMARY KEY,
    session_id TEXT,
    user_id TEXT,
    turn_no INTEGER,
    user_input TEXT,
    assistant_response TEXT,
    intent_type TEXT,
    duration_ms REAL,
    prompt_tokens INTEGER,
    completion_tokens INTEGER,
    timestamp DATETIME,
    deleted_at DATETIME
);

-- Indexes for fast retrieval
CREATE INDEX idx_conv_user ON conversations(user_id, timestamp DESC);
CREATE INDEX idx_conv_session ON conversations(session_id, turn_no);
```

#### `facts`
Stores FACTUAL information with full provenance:
```sql
CREATE TABLE facts (
    id INTEGER PRIMARY KEY,
    user_id TEXT,
    content TEXT,              -- Full sentence (not keywords!)
    content_hash TEXT,         -- SHA256 for deduplication
    category TEXT,             -- personal, preference, etc.
    importance_score REAL,     -- 0.0 to 1.0
    
    -- Provenance
    conversation_id INTEGER,   -- Which conversation created this
    message_id TEXT,
    source_doc_id TEXT,
    source_span TEXT,
    
    -- Vector reference
    embedding_id TEXT,         -- Links to ChromaDB
    
    -- Lifecycle
    created_at DATETIME,
    updated_at DATETIME,
    deleted_at DATETIME,
    
    FOREIGN KEY (conversation_id) REFERENCES conversations(id),
    UNIQUE(user_id, content_hash)  -- Prevent duplicates
);
```

#### `facts_fts`
FTS5 full-text search index (auto-synced):
```sql
CREATE VIRTUAL TABLE facts_fts USING fts5(
    content,
    content='facts',
    content_rowid='id'
);

-- Auto-maintained via triggers
```

### ChromaDB Collections

#### Collection: `memory_facts`
Stores embeddings of factual information:
```python
{
    "ids": ["fact_123"],
    "documents": ["User's birthday is March 15, 1990"],
    "embeddings": [[0.1, 0.2, ...]],  # Auto-generated
    "metadatas": [{
        "user_id": "alice",
        "fact_id": 123,  # Links back to SQL
        "category": "personal",
        "importance": 0.9,
        "created_at": "2025-01-01T00:00:00Z"
    }]
}
```

---

## üîç Retrieval Strategies

### 1. Keyword Search (SQL FTS5)
**When to use:** User query has specific keywords

```python
# User asks: "When is my birthday?"
# FTS5 finds: "User's birthday is March 15, 1990"
# Match: "birthday" keyword present
```

**Performance:** <5ms  
**Cost:** FREE

### 2. Semantic Search (Vector)
**When to use:** Conceptual or vague queries

```python
# User asks: "What do I enjoy?"
# Vector finds: "User loves jazz music"
# Match: "enjoy" ‚âà "love" semantically
```

**Performance:** <50ms  
**Cost:** FREE (local embeddings)

### 3. Recency Boost
**When to use:** Context from recent conversation

```python
# User: "What did we just talk about?"
# Gets: Last 3-5 conversation turns
# Useful: Maintains conversation flow
```

### 4. Importance Ranking
**When to use:** Multiple results

```python
# Rank by:
score = (relevance * 0.6) + (importance * 0.3) + (recency * 0.1)

# Personal info (importance=0.9) ranks higher
# than casual preferences (importance=0.5)
```

---

## üíæ Data Flow Example

### Storing a Fact

```
User: "My name is Alice and I was born on March 15, 1990"
    ‚Üì
[1] Classify (OpenAI)
    ‚Üí FACTUAL (importance: 0.9, category: PERSONAL)
    ‚Üì
[2] Store in SQL
    INSERT INTO conversations (...)
    INSERT INTO facts (content="My name is Alice...")
    ‚Üí fact_id = 42
    ‚Üì
[3] Generate embedding (ChromaDB)
    ‚Üí embedding_id = "fact_42"
    ‚Üì
[4] Link back to SQL
    UPDATE facts SET embedding_id = "fact_42" WHERE id = 42
    ‚Üì
‚úÖ Stored in both SQL + Vector!
```

### Retrieving Context

```
User: "What's my name?"
    ‚Üì
[1] FTS5 Search
    SELECT * FROM facts_fts WHERE facts_fts MATCH 'name'
    ‚Üí Finds: "My name is Alice..."
    ‚Üí Score: 0.85 (high keyword match)
    ‚Üì
[2] Vector Search
    vector_store.search("What's my name?")
    ‚Üí Finds: "My name is Alice..."
    ‚Üí Score: 0.92 (high semantic match)
    ‚Üì
[3] Combine & Deduplicate
    ‚Üí 1 unique result (same fact from both sources)
    ‚Üì
[4] Format for AI
    Context: "Relevant information from memory:
             - My name is Alice and I was born on March 15, 1990"
    ‚Üì
[5] Inject into prompt
    System: You are a helpful assistant.
            {context}
    User: What's my name?
    ‚Üì
AI: Your name is Alice! ‚úÖ
```

---

## üìà Performance Characteristics

| Operation | Time | Database | Cost |
|-----------|------|----------|------|
| Classify | 200-500ms | OpenAI API | $0.001 |
| Store SQL | 2-5ms | SQLite | FREE |
| Store Vector | 50-100ms | ChromaDB | FREE |
| FTS Search | 5-10ms | SQLite | FREE |
| Vector Search | 50-100ms | ChromaDB | FREE |
| Format Context | 1-2ms | Local | FREE |

**Total per conversation:** ~300-700ms, ~$0.001

---

## üí∞ Cost Breakdown

### Classification
- **API**: OpenAI GPT-4o-mini
- **Cost**: ~$0.001 per conversation
- **Frequency**: Every conversation

### Embeddings
- **API**: ChromaDB (local, free!)
- **Cost**: $0 (uses local sentence-transformers)
- **Frequency**: Only FACTUAL conversations (~30%)

### Storage
- **SQL**: SQLite (local, free!)
- **Vector**: ChromaDB (local, free!)
- **Cost**: $0

### Retrieval
- **All**: Local queries
- **Cost**: $0

**Monthly (1000 conversations):**
- Classification: $1.00
- Embeddings: $0.00
- Storage: $0.00
- Retrieval: $0.00
- **Total: ~$1/month** üéâ

---

## üîí Privacy & Data Control

### Soft Delete
```python
# Mark as deleted (reversible)
fact.deleted_at = datetime.now()
# Fact stays in DB but hidden from queries
```

### Hard Delete
```python
# Permanent removal
DELETE FROM facts WHERE id = 123;
vector_store.delete("fact_123");
```

### Export Data
```python
# Get all user data
data = {
    "conversations": get_conversations(user_id),
    "facts": get_facts(user_id),
    "preferences": get_preferences(user_id)
}
json.dump(data, file)
```

### Reset
```python
# Clear everything
sql_store.reset()
vector_store.reset()
```

---

## üéØ Best Practices

### ‚úÖ DO
- Store complete sentences, not keywords
- Use hybrid retrieval (FTS + Vector)
- Set appropriate importance scores
- Track provenance (conversation_id)
- Deduplicate via content hash

### ‚ùå DON'T
- Store just keywords ("Alice", "March 15")
- Only use one retrieval method
- Store ephemeral conversations in vector
- Lose track of where facts came from
- Create duplicate facts

---

## üîß Configuration

### Tune Classification
Adjust prompts in `classifier.py`:
```python
# Make stricter (fewer facts stored)
importance_threshold = 0.7  # Only store if >0.7

# Make looser (more facts stored)
importance_threshold = 0.3  # Store if >0.3
```

### Tune Retrieval
Adjust in `memory_manager.py`:
```python
retrieve_context(
    query=user_input,
    max_results=5,         # Fewer = faster, more = context
    include_recent=True    # Include recent conversations
)
```

### Tune Context Formatting
```python
format_context_for_prompt(
    results,
    max_length=500        # Adjust based on your model
)
```

---

## üìö Further Reading

- `MEMORY_FIX_GUIDE.md` - Troubleshooting recall issues
- `test_memory_phase3.py` - End-to-end test examples
- `memory_cli.py` - CLI tool for inspection
- API docs in each module's docstrings

---

**The memory system gives your assistant true intelligence through persistent, searchable, context-aware storage.** üß†